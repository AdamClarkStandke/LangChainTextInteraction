{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1r9UpmUHhGSCdstpooQhpvd-GiubuBqw4",
      "authorship_tag": "ABX9TyN4F2GuIMAES1Zbgvg2R4SW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamClarkStandke/LangChainTextInteraction/blob/main/InteractivePDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WpKprRqC9-_E"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install -qU \"langchain-chroma>=0.1.2\"\n",
        "!pip install langchain-huggingface\n",
        "!pip install langchain-community\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_chroma import Chroma\n",
        "import getpass\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "cYy1pleS_bfN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load text dataset to play with!!! :)\n",
        "dataset = load_dataset(\n",
        "  \"SammyTime/plaything\",\n",
        "  revision=\"main\"  # tag name, or branch name, or commit hash\n",
        ")\n",
        "print(dataset['train'][:]['text'][:5])"
      ],
      "metadata": {
        "id": "h50gfTlVMm9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_api_key = getpass.getpass(\"Enter your HF Inference API Key:\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyI29P9iWKz6",
        "outputId": "2ae8eb89-d6b0-4c60-8c99-31e38d99c303"
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your HF Inference API Key:\n",
            "\n",
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFaceEmbeddings and HugginFaceEndpoint\n",
        "\n",
        "###HugginFaceEmbeddings\n",
        "\n",
        "As detailed at [SentenceTransformers](https://sbert.net/index.html) creates a sentance embedding space by using Sentence Transformers (a.k.a. SBERT). Sentence Transformers contains over 5,000 pre-trained Sentence Transformers models.\n",
        "\n",
        "The HuggingFace Embeddings takes in the folloing arguments/paramters:\n",
        "\n",
        "1.   model_name_or_path: If it is a filepath on disc, it loads the model from that path. If it is not a path, it first tries to download a pre-trained SentenceTransformer model. If that fails, tries to construct a model from the Hugging Face Hub with that name.\n",
        "2.   device: Device (like “cuda”, “cpu”, “mps”, “npu”) that should be used for computation.\n",
        "3.   similarity_fn_name:The name of the similarity function to use. Valid options are “cosine”, “dot”, “euclidean”, and “manhattan”. If not set, it is automatically set to “cosine” if similarity or similarity_pairwise are called while model.similarity_fn_name is still None.\n",
        "4.   model_kwargs: Additional model configuration parameters to be passed to the Huggingface Transformers model.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###HuggingFaceEndpoint\n",
        "\n",
        "As detailed at [LangChain](https://python.langchain.com/v0.2/docs/integrations/llms/huggingface_endpoint/) HuggingFace Endpoints interacts with the [Hugging Face Hub](https://huggingface.co/docs/hub/index) a platform with over 120k models and 20K datasets (also access to spaces which hosts ml applications). HuggingFace endpoint interacts with the [Severless Endpoint API](https://huggingface.co/docs/api-inference/en/index) to get access to over 150,000 publicly accessible machine learning models, or your own private models, via simple HTTP requests, with fast inference hosted on Hugging Face shared infrastructure (there is also a [dedicated Endpoint](https://huggingface.co/docs/inference-endpoints/en/index) for enterprise workloads) 🍫\n",
        "\n",
        "After taking the course [Developing LLM Applications with LangChain](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.datacamp.com%2Fcourses%2Fdeveloping-llm-applications-with-langchain&psig=AOvVaw3HVd5sI5Y0BO077PdAbWMI&ust=1724691156100000&source=images&cd=vfe&opi=89978449&ved=0CAYQrpoMahcKEwjo4unrzZCIAxUAAAAAHQAAAAAQBA) as taught by Jonathan Bennion and James Chapman I decided to try [Retrieval-Augmented Generation (aka RAG)](https://arxiv.org/pdf/2005.11401) using LangChain. As detailed by the authors of the RAG method:\n",
        "\n",
        "> Large pre-trained language models have been shown to store factual knowledge\n",
        "in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks...[w]e explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.\n",
        "\n",
        "The HuggingFace Endpoint takes in the folloing arguments/parmaters:\n",
        "\n",
        "\n",
        "1.   model: This is the HuggingFace repository to download the pre-trained LLM model to use. The default model is [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct).\n",
        "2.   max_new_tokens: the maximum number of tokens to generate. In other words, the size of the output sequence, not including the tokens in the prompt. The default is 512.\n",
        "3.   top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Is an int and the default value is None. See [top-K sampeling](https://arxiv.org/pdf/1904.09751) (has no max)\n",
        "4.   top_p: If set to < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation. Is a float and the default value is 0.95. See [nucleus sampling](https://arxiv.org/pdf/1904.09751) (range 0 to 1)\n",
        "5.   typical_p: Typical Decoding mass. See [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666) for more information. Is a float and the default value is  0.95. (range 0 to 1)\n",
        "6.   temperature: The value used to module the logits distribution. Is a float and the default value is 0.8. (must be greater than 0, has no max)\n",
        "7.   repetition_penalty: The parameter for repetition penalty. 1.0 means no penalty. See [theta in Penalized Sampeling](https://arxiv.org/pdf/1909.05858.pdf) for more details. Is a float and the default value is 1.2. (has no max)\n",
        "8.   return_full_text: Whether to prepend the prompt to the generated text\n",
        "9.   seed: Random sampling seed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0oMNr8UgJDg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HuggingFaceEmbeddings Parameters\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': True}\n",
        "# Embedding space instantiation\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "  model_name=model_name,\n",
        "  model_kwargs=model_kwargs,\n",
        "  encode_kwargs=encode_kwargs\n",
        ")\n",
        "# HuggingFaceEndpoint LLM Parameters\n",
        "model = 'tiiuae/falcon-7b-instruct'\n",
        "max_new_tokens= 512\n",
        "top_k= None\n",
        "top_p=0.95\n",
        "typical_p = 0.95\n",
        "temperature = 0.8\n",
        "repetition_penalty = 1.2\n",
        "return_full_text=True\n",
        "seed= 42\n",
        "# LLM model instantiation\n",
        "llm = HuggingFaceEndpoint(repo_id=model, huggingfacehub_api_token=inference_api_key,\n",
        "                          max_new_tokens = max_new_tokens,\n",
        "                          top_k = top_k,\n",
        "                          top_p = top_p,\n",
        "                          typical_p = typical_p,\n",
        "                          temperature = temperature,\n",
        "                          repetition_penalty = repetition_penalty,\n",
        "                          return_full_text = return_full_text,\n",
        "                          seed= seed\n",
        "                          )"
      ],
      "metadata": {
        "id": "pp-kt0S0obAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding documents to Chroma Vector Database\n",
        "vectorstore = Chroma.from_texts(dataset['train'][:]['text'], embedding=embeddings)\n",
        "retriever = vectorstore.as_retriever(search_type='similarity', search_kwargs={'k':3})"
      ],
      "metadata": {
        "id": "c4O3q6iuZQgQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Chat Prompt Template for Retrevial Augmented Generation (RAG) for RAG-Langchain Pipeline\n",
        "message = \"\"\"\n",
        "Answer the following question using the context provided:\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages([('human', message)])"
      ],
      "metadata": {
        "id": "Cjit2MFJex_O"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Question = \"what make unmanned drones unmanned\"  #@param { type: \"string\" }\n",
        "# Creating RAG-Langchain to link the retriever, prompt, and llm\n",
        "rag_chain = ({'context': retriever, \"question\": RunnablePassthrough()} | prompt | llm)\n",
        "# Invoking RAG-LangChain by passing in a question regarding the document\n",
        "response = rag_chain.invoke(Question)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "W1EanoSn0nZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "202dc520-a78d-4804-b72a-5f470c448e8c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: \n",
            "Answer the following question using the context provided:\n",
            "\n",
            "Context:\n",
            "[Document(page_content='Keywords —drones, unmanned aircraft system, BVLOS, '), Document(page_content='Keywords —drones, unmanned aircraft system, BVLOS, '), Document(page_content='automated BVLOS drones,” Unmanned Systems Technology . ')]\n",
            "\n",
            "Question:\n",
            "what make unmanned drones unmanned\n",
            "\n",
            "Answer:\n",
            "Unmanned drones are unmanned because they do not have a human operator onboard. The term \"unmanned\" refers to the fact that they are operated without the direct supervision of a human operator.\n"
          ]
        }
      ]
    }
  ]
}